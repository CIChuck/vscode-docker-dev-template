{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPEN_API_KEY: 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\n",
      "Current date and time:2024-04-11 11:09:59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import utilities as ut\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "assert OPENAI_API_KEY is not None , \"OPENAI API Key must be included in the .env file within project root directory\"\n",
    "\n",
    "print(f\"OPEN_API_KEY: {OPENAI_API_KEY}\")\n",
    "print(ut.time_stamp())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim\n"
     ]
    }
   ],
   "source": [
    "import utilities\n",
    "\n",
    "li = utilities.LoremIpsumGenerator()\n",
    "print(li.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm\n",
    "import utilities as u\n",
    "\n",
    "li = u.LoremIpsumGenerator()\n",
    "\n",
    "prompt = \"Once upon a time,\"\n",
    "\n",
    "a = llm.OAI()   \n",
    "\n",
    "response= a.get_chatresponse(prompt=\"What is life?\")\n",
    "print(response)\n",
    " \n",
    "response = a.get_chatresponse(prompt = \"Please complete the following:\"+li.generate())\n",
    "print(response)\n",
    "\n",
    "Joker = \"you are a professional comedian who tells jokes in the style of Steven Wright\" \n",
    "\n",
    "jokester = llm.OAI(systemPrompt=Joker)\n",
    "p = \"create a list of 5 one liner jokes, list each in JSON format\"\n",
    "\n",
    "response = a.get_JSON_response(prompt = p, max_tokens= 1000)\n",
    "print(response)\n",
    "\n",
    "response = a.get_chatresponse(prompt=\"who won the World Series last year?\")\n",
    "print(response)\n",
    "\n",
    "print(utilities.time_stamp())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CompletionUsage' object has no attribute 'input_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/my_python_dev/lib/python3.11/site-packages/pydantic/main.py:759\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpydantic_extra\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mu\u001b[39;00m\n\u001b[1;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mOAI()   \n\u001b[0;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chatresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is life? keep it short\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/vsc-docker-github/tests/llm.py:35\u001b[0m, in \u001b[0;36mOAI.get_chatresponse\u001b[0;34m(self, prompt, temperature, max_tokens)\u001b[0m\n\u001b[1;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     26\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m max_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m         ]\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#object accounting\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtot_tokens_in \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tokens\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_in \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39minput_tokens\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtot_tokens_out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39moutput_tokens\n",
      "File \u001b[0;32m/opt/my_python_dev/lib/python3.11/site-packages/pydantic/main.py:761\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pydantic_extra[item]\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, item):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CompletionUsage' object has no attribute 'input_tokens'"
     ]
    }
   ],
   "source": [
    "import llm\n",
    "import utilities as u\n",
    "\n",
    "a = llm.OAI()   \n",
    "\n",
    "x,y = a.get_chatresponse(prompt=\"What is life? keep it short\", max_tokens=25)\n",
    "\n",
    "#print(x)\n",
    "#print(a.tot_tokens_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9CuJl2Hok7jlNGshEh20tCzrGG4Xx', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Life is the condition that distinguishes living organisms from inorganic matter, characterized by growth, reproduction, functional activity, and continual', role='assistant', function_call=None, tool_calls=None))], created=1712863449, model='gpt-4-0125-preview', object='chat.completion', system_fingerprint='fp_b77cb481ed', usage=CompletionUsage(completion_tokens=25, prompt_tokens=24, total_tokens=49))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is a complex phenomenon characterized by growth, reproduction, adaptation, and response to stimuli in living organisms.\n"
     ]
    }
   ],
   "source": [
    "import llm\n",
    "import utilities\n",
    "\n",
    "\n",
    "a = llm.Anthropic()  \n",
    "\n",
    "response, tok_in, tok_out = a.get_chatresponse(prompt=\"What is life, keep your response short\", max_tokens=50)\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
