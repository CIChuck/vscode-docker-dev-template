{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Notebook! ðŸ¥³\n",
    "\n",
    "In this notebook, you will learn how to optimize your DSPy program using BootstrapFewShot, BootstrapFewShotWithRandomSearch, BootstrapFewShotOptuna, COPRO, and MIPRO! \n",
    "\n",
    "We will use Cohere's `Command-R+` and `Command-R` LLMs, as well as OpenAI's `GPT-4`. We will log LLM calls and pipeline traces with `Arize Phoenix`, and show how you can use `Weights & Biases` to monitor `BootstrapFewShot` runs, our first step in this integration!\n",
    "\n",
    "We will also of course use the `Weaviate` database, storing and indexing the Weaviate blog posts.\n",
    "\n",
    "\n",
    "A few requirements:\n",
    "1. You'll need a running Weaviate instance\n",
    "    1. You can create a 14-day free cluster on [WCS](https://console.weaviate.cloud/)\n",
    "    2. Or run Weaviate locally (use the `yaml` file in this folder with `docker-compose up -d`)\n",
    "2. Generate Cohere and/or OpenAI API keys\n",
    "3. Installations\n",
    "    1. weaviate-client\n",
    "    2. dspy-ai\n",
    "4. Load your Weaviate cluster with data\n",
    "    1. If you want to use the Weaviate blogs as the dataset, refer to the `Weaviate-Import.ipynb` file in this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect DSPy to our LLMs and Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dspy-ai[cohere]\n",
    "\n",
    "#pip uninstall dspy-ai << Y\n",
    "\n",
    "!pip install cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yzmOsK3sm17HvmWqSVCr9VheZENhUuM8Y3B8cM5J\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import os\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "cohere_api_key = os.environ.get('COHERE_API_KEY')\n",
    "print(cohere_api_key)\n",
    "\n",
    "command_r = dspy.Cohere(model=\"command-r\", max_tokens=4000, api_key=cohere_api_key)\n",
    "command_r_plus = dspy.Cohere(model=\"command-r-plus\", max_tokens=4000, api_key=cohere_api_key)\n",
    "gpt4 = dspy.OpenAI(model=\"gpt-4\", max_tokens=4000)\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local(host='172.22.0.2')\n",
    "retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "dspy.settings.configure(lm=command_r, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello! How can I help you today?']\n",
      "['Hello! How can I assist you today?']\n"
     ]
    }
   ],
   "source": [
    "print(command_r_plus(\"say hello\"))\n",
    "print(gpt4(\"say hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Arize Phoenix Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "resource = Resource(attributes={})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "DSPyInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './WeaviateBlogRAG-0-0-0.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "gold_answers = []\n",
    "queries = []\n",
    "\n",
    "for row in dataset:\n",
    "    gold_answers.append(row[\"gold_answer\"])\n",
    "    queries.append(row[\"query\"])\n",
    "    \n",
    "data = []\n",
    "\n",
    "for i in range(len(gold_answers)):\n",
    "    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs(\"question\"))\n",
    "\n",
    "trainset, devset, testset = data[:25], data[25:35], data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typed LLM Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypedEvaluator(dspy.Signature):\n",
    "    \"\"\"Evaluate the quality of a system's answer to a question according to a given criterion.\"\"\"\n",
    "    \n",
    "    criterion: str = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    question: str = dspy.InputField(desc=\"The question asked to the system.\")\n",
    "    ground_truth_answer: str = dspy.InputField(desc=\"An expert written Ground Truth Answer to the question.\")\n",
    "    predicted_answer: str = dspy.InputField(desc=\"The system's answer to the question.\")\n",
    "    rating: float = dspy.OutputField(desc=\"A float rating between 1 and 5\")\n",
    "\n",
    "\n",
    "def MetricWrapper(gold, pred, trace=None):\n",
    "    alignment_criterion = \"How aligned is the predicted_answer with the ground_truth?\"\n",
    "    return dspy.TypedPredictor(TypedEvaluator)(criterion=alignment_criterion,\n",
    "                                          question=gold.question,\n",
    "                                          ground_truth_answer=gold.gold_answer,\n",
    "                                          predicted_answer=pred.answer).rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the the context and answer the question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Helpful information for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A detailed answer that is supported by the context.\")\n",
    "    \n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=k)\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        with dspy.context(lm=command_r):\n",
    "            pred = self.generate_answer(context=context, question=question).answer\n",
    "        return dspy.Prediction(context=context, answer=pred, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgtm_query = \"What do cross encoders do?\"\n",
    "toy_ground_truth_answer = \"\"\"\n",
    "Cross encoders score the relevance of a document to a query. They are commonly used to rerank documents.\n",
    "\"\"\"\n",
    "lgtm_example = dspy.Example(question=lgtm_query, gold_answer=toy_ground_truth_answer)\n",
    "\n",
    "uncompiled_Prediction = RAG()(lgtm_query)\n",
    "print(f\"LGTM test query: {lgtm_query} \\n \\n \")\n",
    "print(f\"Uncompiled Answer: {uncompiled_Prediction.answer} \\n \\n\")\n",
    "test_example = dspy.Example(question=lgtm_query, gold_answer=toy_ground_truth_answer)\n",
    "test_pred = uncompiled_Prediction\n",
    "llm_metric_rating = MetricWrapper(test_example, test_pred)\n",
    "print(f\"LLM Metric Rating: {llm_metric_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_r.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=4, display_progress=False)\n",
    "\n",
    "uncompiled_score = evaluate(RAG(), metric=MetricWrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Replace with Teacher Model\n",
    "\n",
    "teacher_settings = {\"lm\": command_r_plus}\n",
    "\n",
    "for i in range(1, 4, 1):\n",
    "    teleprompter = BootstrapFewShot(teacher_settings=teacher_settings,\n",
    "                                    metric=MetricWrapper, \n",
    "                                    max_bootstrapped_demos=i, \n",
    "                                    max_rounds=1)\n",
    "    compiled_RAG = teleprompter.compile(RAG(), trainset=trainset)\n",
    "    compiled_RAG_score = evaluate(compiled_RAG, metric=MetricWrapper)\n",
    "    print(f\"\\n\\033[91mCompiled RAG Score at Demos = {i}: {compiled_RAG_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about how you can use Weights & Biases logging for `BootstrapFewShot` runs [here!](https://github.com/weaviate/recipes/blob/main/integrations/weights_and_biases/wandb_logging_RAG_dspy_cohere.ipynb)\n",
    "\n",
    "In [PR #849] to DSPy, we introduce wandb logging in order to see the `metric_val` returned for each bootstrapped example. To motivate the use case, you may have a rating on a scale of 1 to 5 for answers and you only want to use examples that achieve a 5 in your prompt. This is the first of many in our collaborations between Weaviate and Weights & Biases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='./weights_and_biases/RAG-optimization-dashboard.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teacher_settings = {\"lm\": command_r}\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(teacher_settings=teacher_settings,\n",
    "                                                metric=MetricWrapper,\n",
    "                                                max_bootstrapped_demos=2,\n",
    "                                                num_candidate_programs=2)\n",
    "\n",
    "compiled_RAG = teleprompter.compile(RAG(), trainset=trainset)\n",
    "compiled_RAG_score = evaluate(compiled_RAG, metric=MetricWrapper)\n",
    "print(f\"\\n\\033[91mCompiled RAG Score: {compiled_RAG_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teacher_settings = {\"lm\": command_r}\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(teacher_settings=teacher_settings,\n",
    "                                                metric=MetricWrapper,\n",
    "                                                max_bootstrapped_demos=1,\n",
    "                                                num_candidate_programs=5)\n",
    "\n",
    "compiled_RAG = teleprompter.compile(RAG(), trainset=trainset)\n",
    "compiled_RAG_score = evaluate(compiled_RAG, metric=MetricWrapper)\n",
    "print(f\"\\n\\033[91mCompiled RAG Score: {compiled_RAG_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BootstrapFewShotWithOptuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithOptuna\n",
    "\n",
    "teacher_settings = {\"lm\": command_r}\n",
    "\n",
    "teleprompter = BootstrapFewShotWithOptuna(teacher_settings=teacher_settings,\n",
    "                                          metric=MetricWrapper,\n",
    "                                          max_bootstrapped_demos=2,\n",
    "                                          num_candidate_programs=2)\n",
    "\n",
    "compiled_RAG = teleprompter.compile(RAG(), trainset=trainset, max_demos=2)\n",
    "compiled_RAG_score = evaluate(compiled_RAG, metric=MetricWrapper)\n",
    "print(f\"\\n\\033[91mCompiled RAG Score: {compiled_RAG_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import COPRO\n",
    "\n",
    "gpt4 = dspy.OpenAI(model=\"gpt-4-1106-preview\", max_tokens=4000, model_type=\"chat\")\n",
    "\n",
    "COPRO_teleprompter = COPRO(prompt_model=gpt4,\n",
    "                          metric=MetricWrapper,\n",
    "                          breadth=5,\n",
    "                          depth=3,\n",
    "                          init_temperature=0.7,\n",
    "                          verbose=False,\n",
    "                          track_stats=True)\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "COPRO_compiled_RAG = COPRO_teleprompter.compile(RAG(), trainset=trainset[:3], eval_kwargs=kwargs)\n",
    "eval_score = evaluate(COPRO_compiled_RAG, metric=MetricWrapper)\n",
    "print(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(COPRO_compiled_RAG(question=\"What is ref2vec?\").answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command_r.inspect_history(n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typed COPRO\n",
    "\n",
    "[Work in Progress]\n",
    "\n",
    "```python\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluator_for_TypedCOPRO = Evaluate(metric=MetricWrapper, devset=devset, num_threads=4, display_progress=False)\n",
    "\n",
    "from dspy.teleprompt import optimize_signature\n",
    "\n",
    "TypedCOPRO_compiled_RAG = optimize_signature(\n",
    "    student=RAG(),\n",
    "    evaluator=evaluator_for_TypedCOPRO,\n",
    "    n_iterations=10,\n",
    "    sorted_order=\"increasing\",\n",
    "    strategy=\"best\",\n",
    "    max_examples=20,\n",
    "    prompt_model=command_r,\n",
    "    initial_prompts=2,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "eval_score = evaluate(TypedCOPRO_compiled_RAG, metric=MetricWrapper)\n",
    "print(eval_score)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationSummarizer(dspy.Signature):\n",
    "    \"\"\"Given a series of observations I have made about my dataset, please summarize them into a brief 2-3 sentence summary which highlights only the most important details.\"\"\"\n",
    "\n",
    "    observations = dspy.InputField(desc=\"Observations I have made about my dataset\")\n",
    "    summary = dspy.OutputField(\n",
    "        desc=\"Two to Three sentence summary of only the most significant highlights of my observations\",\n",
    "    )\n",
    "\n",
    "\n",
    "class DatasetDescriptor(dspy.Signature):\n",
    "    (\n",
    "        \"\"\"Given several examples from a dataset please write observations about trends that hold for most or all of the samples. \"\"\"\n",
    "        \"\"\"Some areas you may consider in your observations: topics, content, syntax, conciceness, etc. \"\"\"\n",
    "        \"\"\"It will be useful to make an educated guess as to the nature of the task this dataset will enable. Don't be afraid to be creative\"\"\"\n",
    "    )\n",
    "\n",
    "    examples = dspy.InputField(desc=\"Sample data points from the dataset\")\n",
    "    observations = dspy.OutputField(desc=\"Somethings that holds true for most or all of the data you observed\")\n",
    "\n",
    "\n",
    "class DatasetDescriptorWithPriorObservations(dspy.Signature):\n",
    "    (\n",
    "        \"\"\"Given several examples from a dataset please write observations about trends that hold for most or all of the samples. \"\"\"\n",
    "        \"\"\"I will also provide you with a few observations I have already made.  Please add your own observations or if you feel the observations are comprehensive say 'COMPLETE' \"\"\"\n",
    "        \"\"\"Some areas you may consider in your observations: topics, content, syntax, conciceness, etc. \"\"\"\n",
    "        \"\"\"It will be useful to make an educated guess as to the nature of the task this dataset will enable. Don't be afraid to be creative\"\"\"\n",
    "    )\n",
    "\n",
    "    examples = dspy.InputField(desc=\"Sample data points from the dataset\")\n",
    "    prior_observations = dspy.InputField(desc=\"Some prior observations I made about the data\")\n",
    "    observations = dspy.OutputField(\n",
    "        desc=\"Somethings that holds true for most or all of the data you observed or COMPLETE if you have nothing to add\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=gpt4)\n",
    "\n",
    "dataset_descriptor = dspy.Predict(DatasetDescriptor)\n",
    "dataset_descriptor_with_prior = dspy.Predict(DatasetDescriptorWithPriorObservations)\n",
    "observation_summarizer = dspy.Predict(ObservationSummarizer)\n",
    "\n",
    "def examples_to_strings(trainset):\n",
    "    example_strings = []\n",
    "    for example in trainset:\n",
    "        question = example.question\n",
    "        gold_answer = example.gold_answer\n",
    "        example_string = f\"Question: {question}\\nAnswer: {gold_answer}\"\n",
    "        example_strings.append(example_string)\n",
    "    return example_strings\n",
    "\n",
    "batch_size=5\n",
    "dataset_description = \"\"\n",
    "for start_index in range(0, len(trainset), batch_size):\n",
    "    examples = examples_to_strings(trainset[start_index:start_index+batch_size])\n",
    "    examples = \"\".join(examples)\n",
    "    if start_index == 0:\n",
    "        dataset_description = dataset_descriptor(examples=examples).observations\n",
    "    else:\n",
    "        dataset_description = dataset_descriptor_with_prior(examples=examples,\n",
    "                                                           prior_observations=dataset_description).observations\n",
    "    summary = observation_summarizer(observations=dataset_description).summary\n",
    "    print(f\"\\033[32m\\nStart index: {start_index}.\")\n",
    "    print(f\"\\033[0m\\nDatasetDescriptor output: {dataset_description}\")\n",
    "    print(f\"\\033[31m\\nSummarizer output: {summary}\\n\")\n",
    "    dataset_description = summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPRO\n",
    "\n",
    "dspy.settings.configure(lm=command_r)\n",
    "\n",
    "teleprompter = MIPRO(prompt_model=gpt4, task_model=command_r, metric=MetricWrapper, num_candidates=10, init_temperature=0.5)\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "MIPRO_compiled_RAG = teleprompter.compile(RAG(), trainset=trainset, num_trials=3, max_bootstrapped_demos=1, max_labeled_demos=0, eval_kwargs=kwargs)\n",
    "eval_score = evaluate(MIPRO_compiled_RAG, metric=MetricWrapper)\n",
    "print(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MIPRO_compiled_RAG(question=\"What is ref2vec?\").answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command_r.inspect_history(n=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
